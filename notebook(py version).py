# -*- coding: utf-8 -*-
"""notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11HzdIfana4q9UmNlTlN22lxrPDy4QTu5

# Import Library
"""

import pandas as pd
from ucimlrepo import fetch_ucirepo
import matplotlib.pyplot as plt
import prince
from scipy.stats import chi2_contingency
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
from xgboost import XGBClassifier

"""# Datasets Understanding

## Data Loading
"""

phishing_websites = fetch_ucirepo(id=327)

X = phishing_websites.data.features
y = phishing_websites.data.targets

print(phishing_websites.metadata)

print(phishing_websites.variables)

"""menggunakan library ucimlrepo langsung untuk mengambil dataset

Ada 11055 Baris data dalam datasets tersebut, dengan jumlah fitur sebanyak 30

# Analysis Variable
"""

X.info()

unique2 = 0
unique3 = 0
boolean_columns = []

for i in X.columns:
    if X[i].unique().size == 2:
        unique2 += 1
        if set(X[i].unique()) == {0, 1}:
            boolean_columns.append(i)
    else:
        unique3 += 1

print(f"Number 2 Unique Features: {unique2}")
print(f"Number 3 Unique Features: {unique3}")
print(f"Boolean Columns: {boolean_columns}")

"""Data sudah berbentuk numerik dengan dua macam tipe

#### Fitur dengan 2 unique Value
Data dengan tipe ini terdapat sejumlah 22 kolom

#### Fitur dengan 3 Unique Value
Data dengan tipe ini terdapat sejumlah 8 kolom

Nilai -1, 0, dan 1 pada dataset ini tidak selalu merepresentasikan urutan (ordinal) atau level yang konsisten secara semantik. Beberapa fitur menggunakan nilai tersebut untuk menyatakan klasifikasi (legit/suspicious/phishing), sementara terdapat satu fitur yang hanya menyatakan kehadiran atau ketiadaan karakteristik teknis tertentu (boolean) yaitu fitur `redirect`. Oleh karena itu, interpretasi nilai harus dilihat per fitur berdasarkan deskripsi asalnya.
"""

X[X.columns].astype('category').describe()

"""Untuk informasi statistik dataset, saya telah merubah tipe data nya terlebih dahulu agar nilai tidak dianggap sebagai integer, mealainkan sebagai category untuk memudahkan pembacaan informasi statistik

# Handle Missing Value
"""

X.isnull().sum()

"""Tidak terdapat missing value dalam dataset ini

## Handle Outliers and Invalid Data
"""

for i in X.columns:
    if X[i].unique().size > 3:
        print(f"Terdapat outlier di fitur {i}")

"""Untuk pengecekan outlier, digunakan pendekatan yang berbeda karena seluruh fitur dalam dataset adalah fitur kategori, sehingga tidak akan cocok apabila menggunakan teknik IQR sebagai pengecekan outliers. Sebagai gantinya, pengecekan outlier dilakukan dengan cara mengecek jumlah unique value dalam tiap kolom, jika ada yang lebih maka dalam kolom itu terdapat outlier atau nilai yang tidak sesuai

# Univariate Analysis
"""

for i in X.columns:
    count = X[i].value_counts()
    percent = 100 * X[i].value_counts(normalize=True)
    df = pd.DataFrame({'jumlah sampel': count, 'persentase': percent.round(1)})
    print(f"\nKolom: {i}")
    print(df)

    count.plot(kind='bar', title=i)
    plt.xlabel('Nilai')
    plt.ylabel('Jumlah Sampel')
    plt.tight_layout()
    plt.show()

"""Dari hasil visualisasi persebaran value di tiap fitur, hampir seluruh fitur memiliki kondisi imbalance. Nilai fitur lebih condong ke 1 (Legit)

# Multivariate Analysis

## Multiple Correspondence Analysis
"""

mca = prince.MCA(n_components=2, random_state=42)
mca = mca.fit(X)

row_coords = mca.transform(X)

plt.figure(figsize=(8, 6))
scatter = plt.scatter(row_coords[0], row_coords[1], c=y['result'], cmap='coolwarm', alpha=0.5)
plt.title("MCA - Row Coordinates Features (Colored by Label)")
plt.xlabel("Dim 1")
plt.ylabel("Dim 2")
plt.grid(True)
plt.colorbar(scatter, label='Label (result)')
plt.show()

"""- Distribusi berlapis atau terpisah di area tertentu menunjukkan bahwa komponen hasil MCA mampu memetakan perbedaan antara kelas phishing dan non-phishing ke dalam dimensi lebih rendah.

- Clustering warna: Beberapa area didominasi oleh warna merah atau biru, artinya:

    - Observasi dengan label yang sama cenderung terkonsentrasi di wilayah tertentu dalam ruang MCA.

    - Ini menunjukkan bahwa MCA berhasil menangkap struktur diskriminatif dari fitur kategorikal dalam data.

- Overlap sebagian juga terlihat: artinya meskipun ada pemisahan, masih ada tumpang tindih antar kelas. Hal ini umum terjadi pada data kategorikal yang tidak sepenuhnya linier separable.

## Korelasi Fitur dengan Target

### Chi-Square Test of Independence
"""

results = []

for col in X.columns:
    contingency = pd.crosstab(X[col], y['result'])
    chi2, p, dof, expected = chi2_contingency(contingency)

    results.append({
        'Feature': col,
        'Chi-square': chi2,
        'p-value': p,
        'Degrees of Freedom': dof,
        'Significant': 'Yes' if p < 0.05 else 'No'
    })

chi2_df = pd.DataFrame(results)

chi2_df = chi2_df.sort_values(by='p-value')

chi2_df

chi2_df_sorted = chi2_df.sort_values(by='Chi-square', ascending=True)
chi2_df_sorted.plot(x='Feature', y='Chi-square', kind='barh', figsize=(10, 6), legend=False)

"""Untuk mengecek korelasi, pada datasets ini, digunakan teknik **Chi-Square Test of Independence**, yaitu uji statistik yang digunakan untuk menentukan apakah dua variabel kategorikal memiliki hubungan atau saling bebas (independen) satu sama lain.

Teknik ini digunakan dengan alasan bahwa teknik umum seperti korelasi matrix tidak akan cocok jika dataset hanya memiliki tipe category saja

Pada hasil Chi-Square diatas, fitur yang memiliki nilai p kecil, fitur tersebut memiliki hubungan yang signifikan dengan target. Dalam analisis ini, nilai p < 0.05 maka dianggap memiliki hubungan yang signifikan dengan target

# Data Preparation

## Dimension Reduction
"""

mca = prince.MCA(n_components=15, random_state=42)
mca = mca.fit(X)


mca.eigenvalues_summary

"""Multiple Correspondence Analysis (MCA) digunakan untuk mereduksi dimensi data yang seluruh fiturnya bertipe kategori karena metode ini dirancang khusus untuk menangkap pola dan hubungan antar kategori. Berbeda dengan PCA yang hanya cocok untuk data numerik, MCA mampu merepresentasikan data kategorikal ke dalam bentuk numerik berdimensi lebih rendah, sehingga memudahkan visualisasi dan analisis tanpa kehilangan informasi penting dari struktur asli data.

karena setengah fitur yang di reduksi MCA sudah cukup mewakili variance, maka fitur akan di reduksi menjadi 15 fitur dengan MCA
"""

X_reduced = mca.transform(X)
X_reduced

X_reduced.describe()

"""MCA dimulai dengan membuat tabel kontingensi yang menunjukkan frekuensi setiap kemungkinan kombinasi kategori. Hasil MCA tidak perlu standardisasi karena semua data input-nya sudah dalam bentuk kategori (dan MCA berbasis kontingensi, bukan magnitudo nilai).

## Splitting Data
"""

X_train, X_test, y_train, y_test = train_test_split(X_reduced, y['result'], test_size=0.2, random_state=42)

print(f'Total # of sample in whole dataset: {len(X)}')
print(f'Total # of sample in train dataset: {len(X_train)}')
print(f'Total # of sample in test dataset: {len(X_test)}')

"""# Modelling"""

models = pd.DataFrame(index=['train_acc', 'test_acc'],
                      columns=['LogReg', 'RandomForest', 'XGBoost'])

"""Pemilihan Model untuk Data Hasil MCA

Saya memilih Logistic Regression, Random Forest, dan XGBoost karena alasan berikut:

- Logistic Regression: Model sederhana dan cepat, cocok untuk data numerik dengan fitur yang sudah orthogonal dari MCA, serta mudah diinterpretasi.

- Random Forest: Mampu menangani hubungan non-linear dan interaksi antar fitur tanpa banyak pra-pemrosesan, cocok untuk menangkap pola kompleks dari data MCA.

- XGBoost: Model boosting yang efisien dan akurat, sangat baik untuk dataset berdimensi sedang dengan pola non-linear yang rumit.

Ketiga model ini memberikan kombinasi yang baik antara interpretabilitas, kekuatan, dan kemampuan menangkap kompleksitas data hasil MCA.

## Logistic Regression
"""

logreg = LogisticRegression(max_iter=1000, random_state=42)
logreg.fit(X_train, y_train)


models.loc['train_acc', 'LogReg'] = logreg.score(X_train, y_train)
models.loc['test_acc', 'LogReg'] = logreg.score(X_test, y_test)

print(classification_report(y_test, logreg.predict(X_test)))

"""## Random Forest"""

rf = RandomForestClassifier(random_state=42)
rf.fit(X_train, y_train)

models.loc['train_acc', 'RandomForest'] = rf.score(X_train, y_train)
models.loc['test_acc', 'RandomForest'] = rf.score(X_test, y_test)

print(classification_report(y_test, rf.predict(X_test)))

"""## Xgboost"""

y_train_xgb = y_train.replace(-1, 0)
y_test_xgb = y_test.replace(-1, 0)

xgboost = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
xgboost.fit(X_train, y_train_xgb)

models.loc['train_acc', 'XGBoost'] = xgboost.score(X_train, y_train_xgb)
models.loc['test_acc', 'XGBoost'] = xgboost.score(X_test, y_test_xgb)


print(classification_report(y_test_xgb, xgboost.predict(X_test)))

"""# Evaluation"""

models

"""dari hasil evaluasi dan classification report, dapat dilihat bahwa baseline model dapat memprediksi dengan baik dataset test

# Feature Importance
"""

rf = RandomForestClassifier(random_state=42)
rf.fit(X_train, y_train)


importances = rf.feature_importances_
feature_names = X_reduced.columns

feat_imp_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})
feat_imp_df = feat_imp_df.sort_values(by='Importance', ascending=False)

# Plot
plt.figure(figsize=(10, 6))
plt.barh(feat_imp_df['Feature'].astype(str), feat_imp_df['Importance'])
plt.xlabel('Importance')
plt.ylabel('MCA Component')
plt.title('Random Forest Feature Importances (MCA Components)')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

feat_imp_df

"""feature importance diambil dari properties model `feature_importances_`

### Mengecek fitur yang paling berkontribusi di componen 2 MCA
"""

pd.set_option('display.max_rows', None)
col_coords = mca.column_coordinates(X)
component_2 = col_coords.iloc[:, 1]
sorted_component_2 = component_2.abs().sort_values(ascending=False)

# Buat DataFrame hasil
result_df = pd.DataFrame({
    'Feature': sorted_component_2.index,
    'Abs(Component 2)': sorted_component_2.values
})

print(result_df)

"""Untuk melihat fitur apa saja yang berkontribusi pada `component 2`, menggunakan koordinat kolom dan diambil hanya component 2 saja"""
"""Dapat dilihat bahwa double slash dan shortining service memiliki kontribusi besar di componen 2, sehingga dapat menjadi indikasi bahwa fitur-fitur tersebut berperan penting dalam membedakan antara URL phishing dan non-phishing. Oleh karena itu, fitur-fitur ini layak mendapat perhatian lebih dalam proses deteksi phishing, baik untuk analisis lanjutan maupun pengembangan model yang lebih akurat dan interpretatif."""